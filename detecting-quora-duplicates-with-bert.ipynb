{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/quora-question-pairs/test.csv.zip\n/kaggle/input/quora-question-pairs/test.csv\n/kaggle/input/quora-question-pairs/train.csv.zip\n/kaggle/input/quora-question-pairs/sample_submission.csv.zip\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Import necessary modules."},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers","execution_count":2,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformers.__version__","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"'3.0.2'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Load data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/quora-question-pairs/train.csv.zip') # use 100k samples for training\ndf.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   id  qid1  qid2                                          question1  \\\n0   0     1     2  What is the step by step guide to invest in sh...   \n1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n2   2     5     6  How can I increase the speed of my internet co...   \n3   3     7     8  Why am I mentally very lonely? How can I solve...   \n4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n\n                                           question2  is_duplicate  \n0  What is the step by step guide to invest in sh...             0  \n1  What would happen if the Indian government sto...             0  \n2  How can Internet speed be increased by hacking...             0  \n3  Find the remainder when [math]23^{24}[/math] i...             0  \n4            Which fish would survive in salt water?             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n      <td>What would happen if the Indian government sto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>6</td>\n      <td>How can I increase the speed of my internet co...</td>\n      <td>How can Internet speed be increased by hacking...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n      <td>8</td>\n      <td>Why am I mentally very lonely? How can I solve...</td>\n      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9</td>\n      <td>10</td>\n      <td>Which one dissolve in water quikly sugar, salt...</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                  id           qid1           qid2   is_duplicate\ncount  404290.000000  404290.000000  404290.000000  404290.000000\nmean   202144.500000  217243.942418  220955.655337       0.369198\nstd    116708.614502  157751.700002  159903.182629       0.482588\nmin         0.000000       1.000000       2.000000       0.000000\n25%    101072.250000   74437.500000   74727.000000       0.000000\n50%    202144.500000  192182.000000  197052.000000       0.000000\n75%    303216.750000  346573.500000  354692.500000       1.000000\nmax    404289.000000  537932.000000  537933.000000       1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>404290.000000</td>\n      <td>404290.000000</td>\n      <td>404290.000000</td>\n      <td>404290.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>202144.500000</td>\n      <td>217243.942418</td>\n      <td>220955.655337</td>\n      <td>0.369198</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>116708.614502</td>\n      <td>157751.700002</td>\n      <td>159903.182629</td>\n      <td>0.482588</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>101072.250000</td>\n      <td>74437.500000</td>\n      <td>74727.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>202144.500000</td>\n      <td>192182.000000</td>\n      <td>197052.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>303216.750000</td>\n      <td>346573.500000</td>\n      <td>354692.500000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>404289.000000</td>\n      <td>537932.000000</td>\n      <td>537933.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"id              0\nqid1            0\nqid2            0\nquestion1       1\nquestion2       2\nis_duplicate    0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(axis=0, inplace=True)\ndf.isnull().sum()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"id              0\nqid1            0\nqid2            0\nquestion1       0\nquestion2       0\nis_duplicate    0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"measurer = np.vectorize(len)\ndict(zip(df, measurer(df.values.astype(str)).max(axis=0)))","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"{'id': 6,\n 'qid1': 6,\n 'qid2': 6,\n 'question1': 623,\n 'question2': 1169,\n 'is_duplicate': 1}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Let's split our dataset into training and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"404287"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df, test_df = df.loc[:0.7 * len(df)], df.loc[0.7 * len(df):0.9 * len(df)], df.loc[0.9 * len(df):]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(282999, 6)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df.shape","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"(80857, 6)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(40431, 6)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Target Distribution\")\nprint(train_df.is_duplicate.value_counts())","execution_count":13,"outputs":[{"output_type":"stream","text":"Train Target Distribution\n0    177877\n1    105122\nName: is_duplicate, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Validation Target Distribution\")\nprint(val_df.is_duplicate.value_counts())","execution_count":14,"outputs":[{"output_type":"stream","text":"Validation Target Distribution\n0    50805\n1    30052\nName: is_duplicate, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Target Distribution\")\nprint(test_df.is_duplicate.value_counts())","execution_count":15,"outputs":[{"output_type":"stream","text":"Test Target Distribution\n0    26342\n1    14089\nName: is_duplicate, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = tf.keras.utils.to_categorical(train_df.is_duplicate, num_classes=2)\n\ny_val = tf.keras.utils.to_categorical(val_df.is_duplicate, num_classes=2)\n\ny_test = tf.keras.utils.to_categorical(test_df.is_duplicate, num_classes=2)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"(282999, 2)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Custom data generator used from keras example."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 128  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2\n\n\nlabels = ['not duplicate', 'duplicate']","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n    \"\"\"Generates batches of data.\n\n    Args:\n        sentence_pairs: Array of premise and hypothesis input sentences.\n        labels: Array of labels.\n        batch_size: Integer batch size.\n        shuffle: boolean, whether to shuffle the data.\n        include_targets: boolean, whether to incude the labels.\n\n    Returns:\n        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n        (or just `[input_ids, attention_mask, `token_type_ids]`\n         if `include_targets=False`)\n    \"\"\"\n\n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n        # Convert batch of encoded features to numpy array.\n        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        # Set to true if data generator is used for training/validation.\n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n    # Freeze the BERT model to reuse the pretrained features without modifying them.\n    bert_model.trainable = False\n\n    sequence_output, pooled_output = bert_model(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, return_sequences=True)\n    )(sequence_output)\n    # Applying hybrid pooling approach to bi_lstm sequence output.\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout)\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )\n","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f6c367da214bedb8ea3ad926a255ec"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0833f91a014a2da215e7f44d0e27c4"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Strategy: {strategy}\")\nmodel.summary()","execution_count":21,"outputs":[{"output_type":"stream","text":"Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f0e25b141d0>\nModel: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 128)]        0                                            \n__________________________________________________________________________________________________\nattention_masks (InputLayer)    [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntoken_type_ids (InputLayer)     [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n                                                                 attention_masks[0][0]            \n                                                                 token_type_ids[0][0]             \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 128, 128)     426496      tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 2)            514         dropout_37[0][0]                 \n==================================================================================================\nTotal params: 109,909,250\nTrainable params: 427,010\nNon-trainable params: 109,482,240\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = BertSemanticDataGenerator(\n    train_df[[\"question1\", \"question2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    val_df[[\"question1\", \"question2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)\n","execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4b5349527f4d3888fc8361e55483d0"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n)","execution_count":23,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n8843/8843 [==============================] - 2148s 243ms/step - loss: 0.3934 - acc: 0.8101 - val_loss: 0.3406 - val_acc: 0.8420\nEpoch 2/2\n8843/8843 [==============================] - 2138s 242ms/step - loss: 0.3458 - acc: 0.8398 - val_loss: 0.3192 - val_acc: 0.8539\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The above model froze the pre-trained model in order to reuse the pretrained features without modifying them.\n\nThe next thing to do is to fine-tune the model by unfreezing the BERT model and retraining with a very low rate. This can deliver meaningful improvement by incrementally adapting the pretrained features to the new data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unfreeze the bert_model.\nbert_model.trainable = True\n# Recompile the model to make the change effective.\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()","execution_count":24,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 128)]        0                                            \n__________________________________________________________________________________________________\nattention_masks (InputLayer)    [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntoken_type_ids (InputLayer)     [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n                                                                 attention_masks[0][0]            \n                                                                 token_type_ids[0][0]             \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 128, 128)     426496      tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 2)            514         dropout_37[0][0]                 \n==================================================================================================\nTotal params: 109,909,250\nTrainable params: 109,909,250\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs\n)","execution_count":26,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n8843/8843 [==============================] - 4692s 531ms/step - loss: 0.2779 - accuracy: 0.8786 - val_loss: 0.2553 - val_accuracy: 0.8914\nEpoch 2/2\n8843/8843 [==============================] - 4688s 530ms/step - loss: 0.2094 - accuracy: 0.9132 - val_loss: 0.2437 - val_accuracy: 0.8999\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = BertSemanticDataGenerator(\n    test_df[[\"question1\", \"question2\"]].values.astype(\"str\"),\n    y_test,\n    batch_size=batch_size,\n    shuffle=False,\n)\nmodel.evaluate(test_data, verbose=1)","execution_count":27,"outputs":[{"output_type":"stream","text":"1263/1263 [==============================] - 221s 175ms/step - loss: 0.2420 - accuracy: 0.8986\n","name":"stdout"},{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"[0.24202945828437805, 0.8986292481422424]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We have achieved 89% accuracy on the test data. Let's try out some sentences:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_similarity(sentence1, sentence2):\n    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n    test_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\n    proba = model.predict(test_data)[0]\n    idx = np.argmax(proba)\n    proba = f\"{proba[idx] * 100: .2f}%\"\n    pred = labels[idx]\n    return pred, proba","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q1 = \"How old is Mark??\"\nq2 = \"What is Mark's Age?\"\n\ncheck_similarity(q1, q2)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"('duplicate', ' 85.29%')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('Quora_duplicate_model', save_format='tf')","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -r 'Quora_duplicate_model.zip' './Quora_duplicate_model'","execution_count":37,"outputs":[{"output_type":"stream","text":"  adding: Quora_duplicate_model/ (stored 0%)\n  adding: Quora_duplicate_model/assets/ (stored 0%)\n  adding: Quora_duplicate_model/variables/ (stored 0%)\n  adding: Quora_duplicate_model/variables/variables.data-00000-of-00001 (deflated 12%)\n  adding: Quora_duplicate_model/variables/variables.index (deflated 80%)\n  adding: Quora_duplicate_model/saved_model.pb (deflated 92%)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -r 'Quora_duplicate_model_ZZ.zip' './Quora_duplicate_model'","execution_count":38,"outputs":[{"output_type":"stream","text":"  adding: Quora_duplicate_model/ (stored 0%)\n  adding: Quora_duplicate_model/assets/ (stored 0%)\n  adding: Quora_duplicate_model/variables/ (stored 0%)\n  adding: Quora_duplicate_model/variables/variables.data-00000-of-00001 (deflated 12%)\n  adding: Quora_duplicate_model/variables/variables.index (deflated 80%)\n  adding: Quora_duplicate_model/saved_model.pb (deflated 92%)\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}